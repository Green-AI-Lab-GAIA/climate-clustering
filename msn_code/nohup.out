INFO:root:called-params ./configs/msn_vits16.yaml
INFO:root:loaded params...
{   'criterion': {   'batch_size': 64,
                     'ent_weight': 0.0,
                     'final_sharpen': 0.25,
                     'me_max': True,
                     'memax_weight': 1.0,
                     'num_proto': 60,
                     'start_sharpen': 0.25,
                     'temperature': 0.1,
                     'use_ent': True,
                     'use_sinkhorn': True},
    'data': {   'color_jitter_strength': 0.5,
                'focal_size': 24,
                'focal_views': 10,
                'image_folder': '../data/training_patches/temperature_3p.npy',
                'label_smoothing': 0.0,
                'num_workers': 10,
                'patch_drop': 0.15,
                'pin_mem': True,
                'rand_size': 32,
                'rand_views': 1,
                'root_path': '/datasets01/'},
    'logging': {   'folder': './checkpoint/msn-temperature-3p',
                   'write_tag': 'msn-temperature-3p'},
    'meta': {   'bottleneck': 1,
                'copy_data': False,
                'drop_path_rate': 0.0,
                'hidden_dim': 2048,
                'load_checkpoint': True,
                'model_name': 'deit_small',
                'output_dim': 256,
                'read_checkpoint': 'msn-temperature-3p-ep75.pth.tar',
                'use_bn': True,
                'use_fp16': False,
                'use_pred_head': False},
    'optimization': {   'clip_grad': 3.0,
                        'epochs': 600,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'lr': 0.001,
                        'start_lr': 0.0002,
                        'warmup': 15,
                        'weight_decay': 0.04}}
INFO:root:Running... (rank: 0/1)
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): MLP(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (fc): Sequential(
    (fc1): Linear(in_features=384, out_features=2048, bias=True)
    (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (gelu1): GELU(approximate='none')
    (fc2): Linear(in_features=2048, out_features=2048, bias=True)
    (bn2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (gelu2): GELU(approximate='none')
    (fc3): Linear(in_features=2048, out_features=256, bias=True)
  )
)
INFO:root:making data transforms
INFO:root:iterations per epoch: 5615
INFO:root:Created prototypes: torch.Size([60, 256])
INFO:root:Requires grad: True
INFO:root:Using AdamW
/home/livia/Documentos/Projetos/climate-clustering/msn_code/src/msn_train.py:443: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(r_path, map_location=torch.device('cpu'))
INFO:root:loaded pretrained encoder from epoch 74 with msg: <All keys matched successfully>
['encoder', 'opt', 'prototypes', 'target_encoder', 'epoch', 'loss', 'batch_size', 'world_size', 'lr', 'temperature']
INFO:root:loaded pretrained encoder from epoch 74 with msg: <All keys matched successfully>
INFO:root:loaded prototypes from epoch 74
INFO:root:loaded optimizers from epoch 74
INFO:root:read-path: ./checkpoint/msn-temperature-3p/msn-temperature-3p-ep75.pth.tar
  0%|          | 0/526 [00:00<?, ?it/s]INFO:root:Epoch 75
INFO:root:[75,     0] loss: 2.095 (2.060 0.035 2.273) (np: 52.0, max-t: 0.750) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (462 ms; 0 ms)
INFO:root:[75,     0] grad_stats: [3.15e-01 1.06e-02] (2.13e-07, 7.31e+00)
INFO:root:[75,    10] loss: 2.203 (2.155 0.048 2.248) (np: 52.5, max-t: 0.736) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (101 ms; 0 ms)
INFO:root:[75,    10] grad_stats: [2.87e-01 1.85e-02] (1.70e-07, 8.70e+00)
INFO:root:[75,    20] loss: 2.185 (2.136 0.050 2.246) (np: 52.2, max-t: 0.741) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (83 ms; 0 ms)
INFO:root:[75,    20] grad_stats: [3.77e-01 1.93e-02] (2.00e-07, 7.88e+00)
INFO:root:[75,    30] loss: 2.182 (2.135 0.048 2.247) (np: 52.1, max-t: 0.742) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (77 ms; 0 ms)
INFO:root:[75,    30] grad_stats: [2.96e-01 1.79e-02] (2.88e-07, 9.28e+00)
INFO:root:[75,    40] loss: 2.173 (2.124 0.049 2.247) (np: 52.0, max-t: 0.740) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (74 ms; 0 ms)
INFO:root:[75,    40] grad_stats: [3.87e-01 7.47e-02] (6.07e-07, 1.14e+01)
INFO:root:[75,    50] loss: 2.157 (2.107 0.050 2.241) (np: 52.1, max-t: 0.742) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (72 ms; 0 ms)
INFO:root:[75,    50] grad_stats: [3.42e-01 1.18e-02] (2.52e-07, 7.38e+00)
INFO:root:[75,    60] loss: 2.151 (2.100 0.051 2.235) (np: 52.0, max-t: 0.742) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (71 ms; 0 ms)
INFO:root:[75,    60] grad_stats: [4.34e-01 4.45e-02] (4.61e-07, 1.41e+01)
INFO:root:[75,    70] loss: 2.143 (2.092 0.051 2.231) (np: 52.0, max-t: 0.744) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (70 ms; 0 ms)
INFO:root:[75,    70] grad_stats: [2.83e-01 2.23e-02] (2.06e-07, 1.43e+01)
INFO:root:[75,    80] loss: 2.140 (2.090 0.051 2.228) (np: 51.9, max-t: 0.743) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (69 ms; 0 ms)
INFO:root:[75,    80] grad_stats: [3.04e-01 5.41e-02] (4.29e-07, 1.26e+01)
INFO:root:[75,    90] loss: 2.145 (2.095 0.050 2.229) (np: 51.9, max-t: 0.742) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (68 ms; 0 ms)
INFO:root:[75,    90] grad_stats: [3.20e-01 7.41e-02] (4.64e-07, 1.33e+01)
INFO:root:[75,   100] loss: 2.144 (2.093 0.051 2.227) (np: 51.9, max-t: 0.742) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (67 ms; 0 ms)
INFO:root:[75,   100] grad_stats: [3.04e-01 8.73e-02] (5.37e-07, 1.26e+01)
INFO:root:[75,   110] loss: 2.141 (2.091 0.050 2.225) (np: 51.9, max-t: 0.742) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (67 ms; 0 ms)
INFO:root:[75,   110] grad_stats: [3.26e-01 4.48e-02] (4.88e-07, 8.24e+00)
INFO:root:[75,   120] loss: 2.139 (2.089 0.050 2.225) (np: 51.9, max-t: 0.742) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (66 ms; 0 ms)
INFO:root:[75,   120] grad_stats: [4.11e-01 2.79e-02] (2.33e-07, 8.10e+00)
INFO:root:[75,   130] loss: 2.137 (2.086 0.051 2.224) (np: 51.9, max-t: 0.741) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (66 ms; 0 ms)
INFO:root:[75,   130] grad_stats: [3.72e-01 2.40e-02] (3.46e-07, 8.14e+00)
INFO:root:[75,   140] loss: 2.137 (2.086 0.051 2.223) (np: 51.9, max-t: 0.741) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (66 ms; 0 ms)
INFO:root:[75,   140] grad_stats: [2.93e-01 4.22e-03] (2.54e-07, 1.19e+01)
INFO:root:[75,   150] loss: 2.135 (2.083 0.051 2.218) (np: 51.8, max-t: 0.741) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (65 ms; 0 ms)
INFO:root:[75,   150] grad_stats: [3.01e-01 5.62e-03] (1.41e-07, 8.47e+00)
INFO:root:[75,   160] loss: 2.138 (2.086 0.051 2.219) (np: 51.8, max-t: 0.741) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (65 ms; 0 ms)
INFO:root:[75,   160] grad_stats: [4.02e-01 4.79e-02] (3.70e-07, 1.12e+01)
INFO:root:[75,   170] loss: 2.139 (2.087 0.052 2.220) (np: 51.8, max-t: 0.741) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (65 ms; 0 ms)
INFO:root:[75,   170] grad_stats: [2.22e-01 3.63e-02] (3.67e-07, 8.95e+00)
INFO:root:[75,   180] loss: 2.138 (2.086 0.052 2.219) (np: 51.7, max-t: 0.740) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (65 ms; 0 ms)
INFO:root:[75,   180] grad_stats: [3.37e-01 2.54e-02] (2.89e-07, 1.04e+01)
INFO:root:[75,   190] loss: 2.139 (2.087 0.052 2.218) (np: 51.8, max-t: 0.740) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (65 ms; 0 ms)
INFO:root:[75,   190] grad_stats: [3.19e-01 3.67e-02] (4.38e-07, 9.44e+00)
INFO:root:[75,   200] loss: 2.141 (2.090 0.051 2.220) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   200] grad_stats: [3.46e-01 5.13e-02] (4.86e-07, 8.46e+00)
INFO:root:[75,   210] loss: 2.143 (2.092 0.051 2.220) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   210] grad_stats: [2.87e-01 2.72e-02] (1.99e-07, 1.09e+01)
INFO:root:[75,   220] loss: 2.141 (2.090 0.051 2.220) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   220] grad_stats: [3.06e-01 3.43e-02] (3.44e-07, 8.19e+00)
INFO:root:[75,   230] loss: 2.143 (2.092 0.051 2.221) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   230] grad_stats: [3.70e-01 1.24e-02] (1.76e-07, 1.18e+01)
INFO:root:[75,   240] loss: 2.144 (2.092 0.051 2.221) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   240] grad_stats: [3.31e-01 2.30e-02] (2.65e-07, 1.09e+01)
INFO:root:[75,   250] loss: 2.145 (2.094 0.051 2.222) (np: 51.8, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   250] grad_stats: [4.14e-01 4.49e-03] (1.96e-07, 1.53e+01)
INFO:root:[75,   260] loss: 2.147 (2.095 0.051 2.222) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   260] grad_stats: [2.97e-01 4.25e-03] (2.27e-07, 8.54e+00)
INFO:root:[75,   270] loss: 2.147 (2.096 0.051 2.222) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   270] grad_stats: [5.28e-01 5.02e-02] (3.98e-07, 1.34e+01)
INFO:root:[75,   280] loss: 2.148 (2.097 0.051 2.221) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   280] grad_stats: [3.50e-01 1.88e-02] (2.83e-07, 1.56e+01)
INFO:root:[75,   290] loss: 2.147 (2.095 0.052 2.220) (np: 51.7, max-t: 0.740) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   290] grad_stats: [2.92e-01 5.08e-03] (2.24e-07, 1.35e+01)
INFO:root:[75,   300] loss: 2.148 (2.096 0.052 2.220) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   300] grad_stats: [4.06e-01 3.05e-02] (4.27e-07, 1.29e+01)
INFO:root:[75,   310] loss: 2.146 (2.094 0.052 2.221) (np: 51.6, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   310] grad_stats: [2.81e-01 3.38e-02] (3.39e-07, 1.43e+01)
INFO:root:[75,   320] loss: 2.147 (2.095 0.052 2.221) (np: 51.7, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   320] grad_stats: [2.74e-01 2.51e-02] (1.99e-07, 6.61e+00)
INFO:root:[75,   330] loss: 2.147 (2.095 0.052 2.220) (np: 51.6, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   330] grad_stats: [3.09e-01 2.16e-02] (1.99e-07, 1.08e+01)
INFO:root:[75,   340] loss: 2.148 (2.096 0.052 2.220) (np: 51.6, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   340] grad_stats: [2.80e-01 2.32e-02] (2.20e-07, 1.05e+01)
INFO:root:[75,   350] loss: 2.147 (2.095 0.052 2.220) (np: 51.6, max-t: 0.739) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   350] grad_stats: [3.22e-01 3.66e-02] (3.12e-07, 1.04e+01)
INFO:root:[75,   360] loss: 2.148 (2.096 0.052 2.220) (np: 51.6, max-t: 0.738) [wd: 4.86e-02] [lr: 9.84e-04] [mem: 9.36e+02] (64 ms; 0 ms)
INFO:root:[75,   360] grad_stats: [2.74e-01 1.97e-02] (1.66e-07, 1.09e+01)
